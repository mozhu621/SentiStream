{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a4b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b1edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Word2Vec([['good']*10,['bad']*10],vector_size=20,alpha=0.03,min_count=10,min_alpha=0.0007,window=4,hs=1,sample=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07934e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=2, vector_size=20, alpha=0.03>\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad8a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "init=Word2Vec.load('PLS_c10.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6689e516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f0639320b90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998e6977",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('./yelp_review_polarity_csv/test.csv', header=None)  # , encoding='ISO-8859-1'\n",
    "f.columns = [\"label\", \"review\"]\n",
    "f.loc[f['label'] == 1, 'label'] = 0\n",
    "f.loc[f['label'] == 2, 'label'] = 1\n",
    "\n",
    "true_label = f.label\n",
    "yelp_review = f.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f52293e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 4.889155864715576\n",
      "[['contrary', 'reviews', 'zero', 'complaints', 'service', 'prices', 'getting', 'tire', 'service', 'past', 'years', 'now', 'compared', 'experience', 'places', 'like', 'pep', 'boys', 'guys', 'experienced', 'know', 'they', 're', 'doing', 'nalso', 'place', 'feel', 'like', 'taken', 'advantage', 'of', 'gender', 'other', 'auto', 'mechanics', 'notorious', 'capitalizing', 'ignorance', 'cars', 'sucked', 'bank', 'account', 'dry', 'but', 'here', 'service', 'road', 'coverage', 'explained', 'let', 'decide', 'nand', 'renovated', 'waiting', 'room', 'it', 'looks', 'lot', 'better', 'previous', 'years']]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import time\n",
    "start_time=time.time()\n",
    "clean_data=[remove_stopwords(review)for review in yelp_review]#remove frequent words that does not carry sentiment\n",
    "tokenised_data = [simple_preprocess(line, deacc=True) for line in clean_data] #remove punctuations and lowercase words also tokenise them\n",
    "print('time taken: '+str(time.time()-start_time))\n",
    "print(tokenised_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4be736e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_data=pd.Series(tokenised_data,name='tokenised_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03b9625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train word2vec model: 33.177834033966064\n"
     ]
    }
   ],
   "source": [
    "# WARNING\n",
    "from gensim.models import Word2Vec\n",
    "# Skip-gram model (sg = 1)\n",
    "# size = 20\n",
    "# window = 3\n",
    "# min_count = 1\n",
    "# workers = 3\n",
    "\n",
    "\n",
    "vector_size=20\n",
    "alpha=0.03\n",
    "min_count=10\n",
    "min_alpha=0.0007\n",
    "window=4\n",
    "hs=1\n",
    "sample=0.00001\n",
    "epochs=30\n",
    "\n",
    "\n",
    "word2vec_model_file =  'word2vec' + str(vector_size) + 'tokenised.model'\n",
    "start_time = time()\n",
    "# Train the Word2Vec Model\n",
    "w2v_model = Word2Vec(tokenised_data, min_count = min_count, vector_size = vector_size, window = window, hs=hs,alpha=alpha,min_alpha=min_alpha,sample=sample,epochs=epochs)\n",
    "print(\"Time taken to train word2vec model: \" + str(time() - start_time))\n",
    "w2v_model.save(word2vec_model_file) # can be used by classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a2fa55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_pos = ['love', 'best', 'beautiful', 'great', 'cool', 'awesome', 'wonderful', 'brilliant', 'excellent',\n",
    "                'fantastic']\n",
    "ref_neg = ['bad', 'worst', 'stupid', 'disappointing', 'terrible', 'rubbish', 'boring', 'awful',\n",
    "                'unwatchable', 'awkward']\n",
    "neg_coefficient=0.5\n",
    "pos_coefficient=0.5\n",
    "def predict(tweet,neg_coefficient,pos_coefficient, model,true_ref_neg=ref_neg,true_ref_pos=ref_pos,count=0,confidence=0.5):\n",
    "    sentence = np.zeros(model.vector_size)\n",
    "    counter = 0\n",
    "    cos_sim_bad, cos_sim_good = 0, 0\n",
    "    for words in tweet:\n",
    "        try:\n",
    "            sentence += model.wv[words]  # np.array(list(model.wv[words]) + new_feature)\n",
    "            counter += 1\n",
    "        except:\n",
    "            pass\n",
    "    if counter != 0:\n",
    "        sentence_vec = sentence / counter\n",
    "    k_cur = min(len(true_ref_neg), len(true_ref_pos))\n",
    "    for neg_word in true_ref_neg[:k_cur]:\n",
    "        try:\n",
    "            cos_sim_bad += np.dot(sentence_vec, model.wv[neg_word]) / (np.linalg.norm(sentence_vec) * np.linalg.norm(model.wv[neg_word]))\n",
    "        except:     \n",
    "            pass\n",
    "    for pos_word in true_ref_pos[:k_cur]:\n",
    "        try:\n",
    "            cos_sim_good += np.dot(sentence_vec, model.wv[pos_word]) / (np.linalg.norm(sentence_vec) * np.linalg.norm(model.wv[pos_word]))\n",
    "        except:\n",
    "            pass\n",
    "    if cos_sim_bad - cos_sim_good > confidence:\n",
    "        # while count<10:\n",
    "        #     print('in first if'+str(cos_sim_bad - cos_sim_good))\n",
    "        return  0\n",
    "    elif cos_sim_bad - cos_sim_good < -confidence:\n",
    "        return  1\n",
    "    else:\n",
    "        if cos_sim_bad * neg_coefficient >= cos_sim_good * pos_coefficient:\n",
    "            # if count<10:\n",
    "            #     print('in sec if'+str([cos_sim_bad,cos_sim_good,pos_coefficient,neg_coefficient]))\n",
    "            return  0\n",
    "        else:\n",
    "            return  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fbeb55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_result(tweets, neg_coefficient,pos_coefficient,w2v_mode,count=0):\n",
    "    predictions=[]\n",
    "    for t in range(len(tweets)):\n",
    "        predict_result = predict(tweets[t], neg_coefficient,pos_coefficient,w2v_model,count=count)\n",
    "        predictions.append(predict_result)\n",
    "        neg_coefficient = predictions.count(0) / (predictions.count(1) + predictions.count(0))\n",
    "        pos_coefficient = 1 - neg_coefficient\n",
    "        count += 1\n",
    "        \n",
    "    ans = accuracy_score(true_label, predictions)\n",
    "    # print(predictions.count(0))\n",
    "    # print(predictions.count(1))\n",
    "    # print(predictions[:10])\n",
    "    count += 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7eb620c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8180263157894737\n",
      "32.79794430732727\n",
      "vec size : 20 epochs : 30\n"
     ]
    }
   ],
   "source": [
    "start_time=time()\n",
    "\n",
    "print(classify_result(tokenised_data, neg_coefficient,pos_coefficient,w2v_model))\n",
    "print(time()-start_time)\n",
    "print(\"vec size : \"+str(vector_size),\"epochs : \"+str(epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bffd4",
   "metadata": {},
   "source": [
    "0.8180263157894737\n",
    "32.79794430732727\n",
    "vec size : 20 epochs : 300.8180263157894737\n",
    "32.79794430732727\n",
    "vec size : 20 epochs : 30\n",
    "\n",
    "\n",
    "0.823921052631579\n",
    "29.009666919708252\n",
    "vec size : 20 epochs : 60\n",
    "\n",
    "epoch 60 dim 90</br>\n",
    "0.8549736842105263</br>\n",
    "34.335697412490845</br>\n",
    "\n",
    "epoch 70 dim 90</br>\n",
    "0.856078947368421</br>\n",
    "30.640328407287598</br>\n",
    "\n",
    "epoch 80 dim 90</br>\n",
    "0.8548421052631578</br>\n",
    "27.404643297195435</br>\n",
    "\n",
    "epoch 90 dim 90</br>\n",
    "0.8538157894736842</br>\n",
    "31.939756870269775</br>\n",
    "\n",
    "epoch 90 dim 100</br>\n",
    "0.8587105263157895</br>\n",
    "32.94704008102417</br>\n",
    "\n",
    "epoch 100 dim 100</br>\n",
    "0.8558684210526316</br>\n",
    "29.760011911392212</br>\n",
    "\n",
    "epoch 110 dim 200</br>\n",
    "0.8533157894736842</br>\n",
    "36.04504609107971</br>\n",
    "\n",
    "epoch 100 dim 200 </br>\n",
    "0.8519473684210527</br>\n",
    "29.53914213180542</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d9fe246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f84d93b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38000"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "09ccddc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0827959 , -0.4999122 ,  1.3272713 ,  0.6675552 ,  0.1212782 ,\n",
       "       -0.52269405,  0.5355004 ,  0.18670893, -0.0178439 ,  0.80836487,\n",
       "        0.09435534, -1.005927  , -0.34542146, -0.6518756 ,  0.3240503 ,\n",
       "        0.5868083 ,  0.30133593, -0.613059  , -0.59222174, -0.67930746],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['awful']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910d50c",
   "metadata": {},
   "source": [
    "### Test PLStream accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2177914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# f = pd.read_csv('./yelp_review_polarity_csv/test.csv', header=None)  # , encoding='ISO-8859-1'\n",
    "f = pd.read_csv('train.csv', header=None)  # , encoding='ISO-8859-1'\n",
    "f.columns = [\"label\", \"review\"]\n",
    "f.loc[f['label'] == 1, 'label'] = 0\n",
    "f.loc[f['label'] == 2, 'label'] = 1\n",
    "\n",
    "true_label = f.label\n",
    "yelp_review = f.review\n",
    "\n",
    "start_time=time()\n",
    "clean_data=[remove_stopwords(review)for review in yelp_review]#remove frequent words that does not carry sentiment\n",
    "tokenised_data = [simple_preprocess(line, deacc=True) for line in clean_data] #remove punctuations and lowercase words also tokenise them\n",
    "print('time taken: '+str(time()-start_time))\n",
    "print(tokenised_data[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80906cec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenised_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_87815/1705247699.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcollector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenised_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenised_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcollector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenised_data' is not defined"
     ]
    }
   ],
   "source": [
    "m=Word2Vec([['good']*10,['bad']*10],vector_size=20,alpha=0.03,min_count=10,min_alpha=0.0007,window=4,hs=1,sample=0.00001,epochs=90)\n",
    "\n",
    "\n",
    "vector_size=100\n",
    "alpha=0.03\n",
    "min_count=10\n",
    "min_alpha=0.0007\n",
    "window=4\n",
    "hs=1\n",
    "sample=0.00001\n",
    "epochs=60\n",
    "\n",
    "collector_size=2000\n",
    "print(len(tokenised_data))\n",
    "start=time()\n",
    "for i in range(0,len(tokenised_data[:100000]),collector_size):\n",
    "    \n",
    "    new_sentences=tokenised_data[i:collector_size+i]\n",
    "    labels=true_label[i:collector_size+i]\n",
    "    \n",
    "    m.build_vocab(new_sentences, update=True)  # 1) update vocabulary\n",
    "    m.train(new_sentences,  # 2) incremental training\n",
    "                              total_examples=m.corpus_count,\n",
    "                              epochs=m.epochs)\n",
    "#     m = Word2Vec(tokenised_data, min_count = min_count, vector_size = vector_size, window = window, hs=hs,alpha=alpha,min_alpha=min_alpha,sample=sample,epochs=epochs)\n",
    "\n",
    "    predictions=[]\n",
    "    for sen in new_sentences:\n",
    "        predict_result = predict(sen, neg_coefficient,pos_coefficient,m)\n",
    "        predictions.append(predict_result)\n",
    "        neg_coefficient = predictions.count(0) / (predictions.count(1) + predictions.count(0))\n",
    "        pos_coefficient = 1 - neg_coefficient\n",
    "        \n",
    "    ans = accuracy_score(labels, predictions)\n",
    "    # print(predictions.count(0))\n",
    "    # print(predictions.count(1))\n",
    "    # print(predictions[:10])\n",
    "    print(ans)\n",
    "print(\"time taken: \"+str(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52915d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
