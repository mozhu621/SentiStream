{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz\n",
    "!tar zxvf yelp_review_polarity_csv.tgz\n",
    "!mv yelp_review_polarity_csv/train.csv train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870ef5f4-671b-4c4e-9035-e2c81e33c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import redis\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pyflink.datastream.functions import RuntimeContext, MapFunction\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream import CheckpointingMode\n",
    "from pyflink.datastream.connectors import StreamingFileSink\n",
    "from pyflink.common.serialization import Encoder\n",
    "\n",
    "from utils import process_text_and_generate_tokens, split\n",
    "\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "087a8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class unsupervised_OSA():\n",
    "\n",
    "    def __init__(self, collector_size,with_accuracy=True):\n",
    "        \"\"\"\n",
    "        :param with_accuracy: True if labels are provided to the datastream. default value is True\n",
    "        \"\"\"\n",
    "        self.initial_model = None\n",
    "        self.redis_param = None\n",
    "        self.start_timer = time()\n",
    "        # collection\n",
    "        self.vocabulary = []\n",
    "        self.true_label = []\n",
    "        self.collector = []\n",
    "        self.cleaned_text = []\n",
    "        self.stop_words = stopwords.words('english')\n",
    "        self.collector_size = collector_size\n",
    "\n",
    "        # model pruning\n",
    "        self.LRU_index = ['good', 'bad']\n",
    "        # self.max_index = max(self.LRU_index)\n",
    "        self.LRU_cache_size = 300000\n",
    "        # self.sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "        # model merging\n",
    "        self.flag = True\n",
    "        self.model_to_train = None\n",
    "        self.timer = time()\n",
    "        self.time_to_reset = 30\n",
    "#         self.time_to_reset = 1\n",
    "\n",
    "        # similarity-based classification preparation\n",
    "        self.true_ref_neg = []\n",
    "        self.true_ref_pos = []\n",
    "        self.ref_pos = ['love', 'best', 'beautiful', 'great', 'cool', 'awesome', 'wonderful', 'brilliant', 'excellent',\n",
    "                        'fantastic']\n",
    "        self.ref_neg = ['bad', 'worst', 'stupid', 'disappointing', 'terrible', 'rubbish', 'boring', 'awful',\n",
    "                        'unwatchable', 'awkward']\n",
    "        # self.ref_pos = [self.sno.stem(x) for x in self.ref_pos]\n",
    "        # self.ref_neg = [self.sno.stem(x) for x in self.ref_neg]\n",
    "\n",
    "        # temporal trend detection\n",
    "        self.pos_coefficient = 0.5\n",
    "        self.neg_coefficient = 0.5\n",
    "\n",
    "        # results\n",
    "        self.confidence = 0.5\n",
    "        # self.acc_to_plot = []\n",
    "        # self.acc_to_plot = []\n",
    "        self.predictions = []\n",
    "        self.labelled_dataset = []\n",
    "        self.confidence_list = []\n",
    "        self.with_accuracy = with_accuracy\n",
    "\n",
    "    def open(self):\n",
    "        # redis-server parameters\n",
    "        self.redis_param = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "\n",
    "        # load initial model\n",
    "        self.initial_model = Word2Vec.load('PLS_c10.model')\n",
    "        self.vocabulary = list(self.initial_model.wv.index_to_key)\n",
    "\n",
    "        # save model to redis\n",
    "        self.save_model(self.initial_model)\n",
    "\n",
    "    def save_model(self, model):\n",
    "        self.redis_param = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "        try:\n",
    "            self.redis_param.set('osamodel', pickle.dumps(model, protocol=pickle.HIGHEST_PROTOCOL))\n",
    "        except (redis.exceptions.RedisError, TypeError, Exception):\n",
    "            logging.warning('Unable to save model to Redis server, please check your model')\n",
    "\n",
    "    def load_model(self):\n",
    "        self.redis_param = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "        # try:\n",
    "        called_model = pickle.loads(self.redis_param.get('osamodel'))\n",
    "        return called_model\n",
    "        # except TypeError:\n",
    "        #     logging.info('The model name you entered cannot be found in redis')\n",
    "        # except (redis.exceptions.RedisError, TypeError, Exception):\n",
    "        #     logging.warning('Unable to call the model from Redis server, please check your model')\n",
    "\n",
    "    # tweet preprocessing\n",
    "\n",
    "    def text_to_word_list(self, text):\n",
    "\n",
    "        clean_word_list = process_text_and_generate_tokens(text)\n",
    "\n",
    "        while '' in clean_word_list:\n",
    "            clean_word_list.remove('')\n",
    "        self.cleaned_text.append(clean_word_list)\n",
    "        if len(self.cleaned_text) >= self.collector_size:\n",
    "            # ans = self.update_model(self.cleaned_text)\n",
    "            # return ans\n",
    "            return 'update_model'\n",
    "\n",
    "    def model_prune(self, model):\n",
    "        if len(model.wv.index_to_key) <= self.LRU_cache_size:\n",
    "            return model\n",
    "        else:\n",
    "            word_to_prune = list(self.LRU_index[30000:])\n",
    "            for word in word_to_prune:\n",
    "                k = model.wv.key_to_index[word]\n",
    "                del model.wv.index_to_key[k]\n",
    "                del model.wv.key_to_index[word]\n",
    "            self.vocabulary = list(model.wv.index_to_key)\n",
    "            return model\n",
    "\n",
    "    def get_model_new(self, final_words, final_vectors, final_syn1, final_syn1neg, final_cum_table, corpus_count,\n",
    "                      final_count, final_sample_int, final_code, final_point, model):\n",
    "\n",
    "        model_new = copy.deepcopy(model)\n",
    "        n_words = len(final_words)\n",
    "        model_new.wv.index_to_key = final_words\n",
    "        model_new.wv.key_to_index = {word: idx for idx, word in enumerate(final_words)}\n",
    "        model_new.wv.vectors = final_vectors\n",
    "        model_new.syn1 = final_syn1  # dk why this is important\n",
    "        model_new.syn1neg = final_syn1neg\n",
    "        model_new.syn1 = final_syn1\n",
    "        model_new.syn1neg = final_syn1neg\n",
    "        model_new.cum_table = final_cum_table\n",
    "        model_new.corpus_count = corpus_count\n",
    "        model_new.corpus_total_words = n_words\n",
    "        model_new.wv.expandos['count'] = final_count\n",
    "        model_new.wv.expandos['sample_int'] = final_sample_int\n",
    "        model_new.wv.expandos['code'] = final_code\n",
    "        model_new.wv.expandos['point'] = final_point\n",
    "        return model_new\n",
    "\n",
    "    def model_merge(self, model1, model2):\n",
    "        # prediction or accuracy not merging\n",
    "#         logger.warning('model_merge')\n",
    "        if model1[0] == 'labelled':\n",
    "            # logging.warning(model1)\n",
    "            return model1[0],(model1[1]) + (model2[1])\n",
    "        elif model1[0] == 'acc':\n",
    "            return (float(model1[1]) + float(model2[1])) / 2\n",
    "        # actual merging taking place\n",
    "        elif model1[0] == 'model':\n",
    "            model1 = model1[1]\n",
    "            model2 = model2[1]\n",
    "            words1 = copy.deepcopy(model1.wv.index_to_key)\n",
    "            words2 = copy.deepcopy(model2.wv.index_to_key)\n",
    "            syn1s1 = copy.deepcopy(model1.syn1)\n",
    "            syn1s2 = copy.deepcopy(model2.syn1)\n",
    "            syn1negs1 = copy.deepcopy(model1.syn1neg)\n",
    "            syn1negs2 = copy.deepcopy(model2.syn1neg)\n",
    "            cum_tables1 = copy.deepcopy(model1.cum_table)\n",
    "            cum_tables2 = copy.deepcopy(model2.cum_table)\n",
    "            corpus_count = copy.deepcopy(model1.corpus_count) + copy.deepcopy(model2.corpus_count)\n",
    "            counts1 = copy.deepcopy(model1.wv.expandos['count'])\n",
    "            counts2 = copy.deepcopy(model2.wv.expandos['count'])\n",
    "            sample_ints1 = copy.deepcopy(model1.wv.expandos['sample_int'])\n",
    "            sample_ints2 = copy.deepcopy(model2.wv.expandos['sample_int'])\n",
    "            codes1 = copy.deepcopy(model1.wv.expandos['code'])\n",
    "            codes2 = copy.deepcopy(model2.wv.expandos['code'])\n",
    "            points1 = copy.deepcopy(model1.wv.expandos['point'])\n",
    "            points2 = copy.deepcopy(model2.wv.expandos['point'])\n",
    "            final_words = []\n",
    "            final_vectors = []\n",
    "            final_syn1 = []\n",
    "            final_syn1neg = []\n",
    "            final_cum_table = []\n",
    "            final_count = []\n",
    "            final_sample_int = []\n",
    "            final_code = []\n",
    "            final_point = []\n",
    "            for idx1 in range(len(words1)):\n",
    "                word = words1[idx1]\n",
    "                v1 = model1.wv[word]\n",
    "                syn11 = syn1s1[idx1]\n",
    "                syn1neg1 = syn1negs1[idx1]\n",
    "                cum_table1 = cum_tables1[idx1]\n",
    "                count = counts1[idx1]\n",
    "                sample_int = sample_ints1[idx1]\n",
    "                code = codes1[idx1]\n",
    "                point = points1[idx1]\n",
    "                try:\n",
    "                    idx2 = words2.index(word)\n",
    "                    v2 = model2.wv[word]\n",
    "                    syn12 = syn1s2[idx2]\n",
    "                    syn1neg2 = syn1negs2[idx2]\n",
    "                    cum_table2 = cum_tables2[idx2]\n",
    "                    v = np.mean(np.array([v1, v2]), axis=0)\n",
    "                    syn1 = np.mean(np.array([syn11, syn12]), axis=0)\n",
    "                    syn1neg = np.mean(np.array([syn1neg1, syn1neg2]), axis=0)\n",
    "                    cum_table = np.mean(np.array([cum_table1, cum_table2]), axis=0)\n",
    "                except:\n",
    "                    v = v1\n",
    "                    syn1 = syn11\n",
    "                    syn1neg = syn1neg1\n",
    "                    cum_table = cum_table1\n",
    "                final_words.append(word)\n",
    "                final_vectors.append(list(v))\n",
    "                final_syn1.append(syn1)\n",
    "                final_syn1neg.append(syn1neg)\n",
    "                final_cum_table.append(cum_table)\n",
    "                final_count.append(count)\n",
    "                final_sample_int.append(sample_int)\n",
    "                final_code.append(code)\n",
    "                final_point.append(point)\n",
    "            for idx2 in range(len(words2)):\n",
    "                word = words2[idx2]\n",
    "                if word in final_words:\n",
    "                    continue\n",
    "                v2 = model2.wv[word]\n",
    "                syn12 = syn1s2[idx2]\n",
    "                syn1neg2 = syn1negs2[idx2]\n",
    "                cum_table2 = cum_tables2[idx2]\n",
    "                count = counts2[idx2]\n",
    "                sample_int = sample_ints2[idx2]\n",
    "                code = codes2[idx2]\n",
    "                point = points2[idx2]\n",
    "                try:\n",
    "                    idx1 = words1.index(word)\n",
    "                    v1 = model1.wv[word]\n",
    "                    syn11 = syn1s1[idx1]\n",
    "                    syn1neg1 = syn1negs1[idx1]\n",
    "                    cum_table1 = cum_tables1[idx1]\n",
    "                    v = np.mean(np.array([v1, v2]), axis=0)\n",
    "                    syn1 = np.mean(np.array([syn11, syn12]), axis=0)\n",
    "                    syn1neg = np.mean(np.array([syn1neg1, syn1neg2]), axis=0)\n",
    "                    cum_table = np.mean(np.array([cum_table1, cum_table2]), axis=0)\n",
    "                except:\n",
    "                    v = v2\n",
    "                    syn1 = syn12\n",
    "                    syn1neg = syn1neg2\n",
    "                    cum_table = cum_table2\n",
    "                final_words.append(word)\n",
    "                final_vectors.append(list(v))\n",
    "                final_syn1.append(syn1)\n",
    "                final_syn1neg.append(syn1neg)\n",
    "                final_cum_table.append(cum_table)\n",
    "                final_count.append(count)\n",
    "                final_sample_int.append(sample_int)\n",
    "                final_code.append(code)\n",
    "                final_point.append(point)\n",
    "\n",
    "            model_new = self.get_model_new(final_words, np.array(final_vectors), np.array(final_syn1),\n",
    "                                           np.array(final_syn1neg), final_cum_table, corpus_count,\n",
    "                                           np.array(final_count),\n",
    "                                           np.array(final_sample_int), np.array(final_code), np.array(final_point),\n",
    "                                           model1)\n",
    "            self.save_model(model_new)\n",
    "            self.flag = True\n",
    "#             logging.warning(\"model 1 merge time: \" + str(time() - model1[2]))\n",
    "#             logging.warning(\"model 2 merge time: \" + str(time() - model2[2]))\n",
    "            return model_new\n",
    "\n",
    "    def map(self, tweet):\n",
    "        \"\"\"\n",
    "        :param tweet: expects tweet in the format [index,label,string] or [index,string]\n",
    "        :return: tag,data\n",
    "        \"\"\"\n",
    "        if self.with_accuracy:\n",
    "            content = tweet[2]\n",
    "            self.true_label.append(int(tweet[1]))\n",
    "            self.collector.append((tweet[0], content))\n",
    "        else:\n",
    "            content = tweet[1]\n",
    "            self.collector.append((tweet[0], content))\n",
    "        tokenize_text_done = self.text_to_word_list(content)\n",
    "        if tokenize_text_done == 'update_model':\n",
    "#             logging.warning('in update_model map')\n",
    "#             logging.warning(self.model_to_train)\n",
    "            self.update_model(self.cleaned_text)\n",
    "\n",
    "            classify_result = self.classify_result(self.cleaned_text)\n",
    "            self.cleaned_text = []\n",
    "            self.true_label = []\n",
    "\n",
    "            if time() - self.timer >= self.time_to_reset:  # prune and return model\n",
    "                self.model_to_train = self.model_prune(self.model_to_train)\n",
    "                model_to_merge = ('model', self.model_to_train)\n",
    "                self.timer = time()\n",
    "                return model_to_merge\n",
    "            else:\n",
    "                not_yet = ('labelled', classify_result)\n",
    "                self.labelled_dataset=[] # flush labelled_dataset here\n",
    "                return not_yet\n",
    "        else:\n",
    "            return 'collecting', '1'\n",
    "\n",
    "    def incremental_training(self, new_sentences):\n",
    "        self.model_to_train.build_vocab(new_sentences, update=True)  # 1) update vocabulary\n",
    "        self.model_to_train.train(new_sentences,  # 2) incremental training\n",
    "                                  total_examples=self.model_to_train.corpus_count,\n",
    "                                  epochs=self.model_to_train.epochs)\n",
    "\n",
    "    def update_LRU_index(self):\n",
    "        for word in self.model_to_train.wv.index_to_key:\n",
    "            if word not in self.vocabulary:  # new words\n",
    "                self.LRU_index.insert(0, word)\n",
    "            else:  # duplicate words\n",
    "                self.LRU_index.remove(word)\n",
    "                self.LRU_index.insert(0, word)\n",
    "        self.vocabulary = list(self.model_to_train.wv.index_to_key)\n",
    "\n",
    "    def update_true_ref(self):\n",
    "        if len(self.ref_neg) > 0:\n",
    "            for words in self.ref_neg:\n",
    "                if words in self.model_to_train.wv:\n",
    "                    self.ref_neg.remove(words)\n",
    "                    if words not in self.true_ref_neg:\n",
    "                        self.true_ref_neg.append(words)\n",
    "        if len(self.ref_pos) > 0:\n",
    "            for words in self.ref_pos:\n",
    "                if words in self.model_to_train.wv:\n",
    "                    self.ref_pos.remove(words)\n",
    "                    if words not in self.true_ref_pos:\n",
    "                        self.true_ref_pos.append(words)\n",
    "\n",
    "    def update_model(self, new_sentences):\n",
    "\n",
    "        if self.flag:\n",
    "            self.model_to_train = self.load_model()\n",
    "            self.flag = False\n",
    "        # else:\n",
    "        #     call_model = self.model_to_train\n",
    "\n",
    "        # incremental learning\n",
    "        self.incremental_training(new_sentences)\n",
    "        self.update_LRU_index()\n",
    "        self.update_true_ref()\n",
    "\n",
    "    def classify_result(self, tweets):\n",
    "        for t in range(len(tweets)):\n",
    "            predict_result = self.predict(tweets[t], self.model_to_train)\n",
    "            self.confidence_list.append(predict_result[0])\n",
    "\n",
    "            d = {'neg_coefficient': self.neg_coefficient, 'pos_coefficient': self.pos_coefficient}\n",
    "            if self.with_accuracy:\n",
    "                d['true_label'] = self.true_label[t]\n",
    "            self.labelled_dataset.append([\n",
    "                self.collector[t][0], predict_result[0], predict_result[1], self.collector[t][1], d])\n",
    "            self.predictions.append(predict_result[1])\n",
    "\n",
    "        self.neg_coefficient = self.predictions.count(0) / (self.predictions.count(1) + self.predictions.count(0))\n",
    "        self.pos_coefficient = 1 - self.neg_coefficient\n",
    "        self.collector = []\n",
    "        ans = self.labelled_dataset\n",
    "        # else:\n",
    "        #     ans = accuracy_score(self.true_label, self.predictions)\n",
    "        self.predictions = []\n",
    "        return ans\n",
    "\n",
    "    def predict(self, tweet, model):\n",
    "        sentence = np.zeros(20)\n",
    "        counter = 0\n",
    "        cos_sim_bad, cos_sim_good = 0, 0\n",
    "        for words in tweet:\n",
    "            try:\n",
    "                sentence += model.wv[words]  # np.array(list(model.wv[words]) + new_feature)\n",
    "                counter += 1\n",
    "            except:\n",
    "                pass\n",
    "        if counter != 0:\n",
    "            sentence_vec = sentence / counter\n",
    "        k_cur = min(len(self.true_ref_neg), len(self.true_ref_pos))\n",
    "        for neg_word in self.true_ref_neg[:k_cur]:\n",
    "            try:\n",
    "                cos_sim_bad += dot(sentence_vec, model.wv[neg_word]) / (norm(sentence_vec) * norm(model.wv[neg_word]))\n",
    "            except:\n",
    "                pass\n",
    "        for pos_word in self.true_ref_pos[:k_cur]:\n",
    "            try:\n",
    "                cos_sim_good += dot(sentence_vec, model.wv[pos_word]) / (norm(sentence_vec) * norm(model.wv[pos_word]))\n",
    "            except:\n",
    "                pass\n",
    "        if cos_sim_bad - cos_sim_good > self.confidence:\n",
    "            return cos_sim_bad - cos_sim_good, 0\n",
    "        elif cos_sim_bad - cos_sim_good < -self.confidence:\n",
    "            return cos_sim_good - cos_sim_bad, 1\n",
    "        else:\n",
    "            if cos_sim_bad * self.neg_coefficient >= cos_sim_good * self.pos_coefficient:\n",
    "                return cos_sim_bad - cos_sim_good, 0\n",
    "            else:\n",
    "                return cos_sim_good - cos_sim_bad, 1\n",
    "\n",
    "\n",
    "# def unsupervised_stream(ds, map_parallelism=1, reduce_parallelism=2):\n",
    "#     # ds.print()\n",
    "#     ds = ds.map(unsupervised_OSA()).set_parallelism(map_parallelism)\n",
    "#     ds = ds.filter(lambda x: x[0] != 'collecting')\n",
    "#     ds = ds.key_by(lambda x: x[0], key_type=Types.STRING())\n",
    "#     ds = ds.reduce(lambda x, y: (x[0], unsupervised_OSA().model_merge(x, y))).set_parallelism(reduce_parallelism)\n",
    "#     ds = ds.filter(lambda x: x[0] != 'model').map(lambda x: x[1])\n",
    "#     # ds = ds.map(for_output()).set_parallelism(1))\n",
    "#     ds = ds.flat_map(split)  # always put output_type before passing it to file sink\n",
    "#     # ds = ds.add_sink(StreamingFileSink  # .set_parallelism(2)\n",
    "#     #                  .for_row_format('./output', Encoder.simple_string_encoder())\n",
    "#     #                  .build())\n",
    "#     return ds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3964f-d627-4d0d-bf3a-8a33ccc48907",
   "metadata": {},
   "source": [
    "model has to return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b51c93c8-03ba-442f-86cc-e340bcf7221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UO:\n",
    "    def __init__(self,obj):\n",
    "        self.object=obj\n",
    "        self.stream=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf177f0-f7f5-4b95-bdd4-7f715c377f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the labels of dataset are only used for accuracy computation, since PLStream is unsupervised\n",
    "f = pd.read_csv('./train.csv')  # , encoding='ISO-8859-1'\n",
    "f.columns = [\"label\",\"review\"]\n",
    "\n",
    "# 20,000 data for quick testing\n",
    "def dataset_trunc(n,f):\n",
    "    true_label = list(f.label)[:n]\n",
    "    for i in range(len(true_label)):\n",
    "        if true_label[i] == 1:\n",
    "            true_label[i] = 0\n",
    "        else:\n",
    "            true_label[i] = 1\n",
    "    yelp_review = list(f.review)[:n]\n",
    "    return true_label,yelp_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91786159-5df1-4d40-84c4-763b2e180c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  \n",
    "warnings.filterwarnings(action='ignore',category=UserWarning,module='gensim')  \n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning,module='gensim')\n",
    "from tqdm import tqdm\n",
    "data_stream = []\n",
    "buckets=[]\n",
    "parallelism = 4\n",
    "collector_size=2000\n",
    "n = 560000 \n",
    "model=None\n",
    "pseudo_dataset=[]\n",
    "start_time=time()\n",
    "assert n >= parallelism*collector_size\n",
    "\n",
    "true_label,yelp_review= dataset_trunc(n,f)\n",
    "\n",
    "\n",
    "# unsupervised_OSA\n",
    "for i in range(parallelism):\n",
    "    buckets.append(UO(unsupervised_OSA(collector_size)))\n",
    "    buckets[i].object.open()\n",
    "\n",
    "k=0\n",
    "iteration =0\n",
    "# pbar =tqdm(len(yelp_review))\n",
    "while k<len(yelp_review):\n",
    "    # map\n",
    "    for j in range(len(yelp_review[k:k+collector_size*parallelism])):\n",
    "        bucket=buckets[j%parallelism]\n",
    "        bucket.stream.append(bucket.object.map([k,int(true_label[k]),yelp_review[k]]))\n",
    "        # pbar.update(1)\n",
    "        k+=1\n",
    "    # filter\n",
    "    \n",
    "    for i in range(parallelism):\n",
    "        stream=buckets[i].stream\n",
    "        stream=pd.DataFrame(stream)\n",
    "        stream=stream[stream[0]!='collecting']\n",
    "        stream=stream.values.tolist()\n",
    "        buckets[i].stream=stream\n",
    "        \n",
    "        \n",
    "\n",
    "    # reduce\n",
    "    if iteration ==0:\n",
    "        pseudo_dataset=buckets[0].stream[0]\n",
    "        buckets[0].stream=[]\n",
    "    else:\n",
    "        pseudo_dataset=buckets[0].object.model_merge(pseudo_dataset,buckets[0].stream[0])\n",
    "    model_array=[]\n",
    "    # label merge\n",
    "    \n",
    "    for j in range(0,len(buckets)-1):\n",
    "        if buckets[j+1].stream[0][0] == 'model':\n",
    "            model_array.append(buckets[j+1].stream[0])\n",
    "            buckets[j+1].stream=[]\n",
    "        else:\n",
    "            pseudo_dataset=buckets[j].object.model_merge(pseudo_dataset,buckets[j+1].stream[0])\n",
    "            buckets[j+1].stream=[]\n",
    "    # model merge\n",
    "    \n",
    "    if model_array:\n",
    "        model =model_array[0][1]\n",
    "        for i in range(len(model_array)-1):\n",
    "            model=buckets[0].object.model_merge(('model',model),model_array[i+1])\n",
    "\n",
    "        for bucket in buckets:\n",
    "            bucket.object.model_to_train=model \n",
    "    iteration += 1\n",
    "# pbar.close()\n",
    "pseudo_dataset =pseudo_dataset[1]\n",
    "print(time()-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c202bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pseudo_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((buckets[3].stream[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets[0].object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a0ee6c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4, 2)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,parallelism,collector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "4f22066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 7 >= parallelism+collector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "457dd339-bc53-44ad-9d1b-a5a7ade412e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(parallelism):\n",
    "    stream=buckets[i].stream\n",
    "    stream=pd.DataFrame(stream)\n",
    "    stream=stream[stream[0]!='collecting']\n",
    "    buckets[i].stream=stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "323a3026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckets[3].stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "188e7b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'labelled'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8b4ce350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckets[0].stream.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1e249b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "548a643d-3ef3-4384-9724-82b088cd8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(buckets[0].stream.shape[0]): # rows\n",
    "    for j in range(0,len(buckets),2):# parallelism\n",
    "        nm=buckets[j].object.model_merge(buckets[j].stream.iloc[i],buckets[j+1].stream.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7141ff79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckets[0].stream.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9aa614-33c0-43c8-9427-a432f1e0ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ats=buckets[0].object.model_to_train.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da93d2-d50b-4e41-8b88-fa0b7b0d583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=buckets[0].object.model_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f2f76-a855-40bc-a7ea-a65497cad0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0046c29-132a-440d-a6dd-00f5e437a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.wv.expandos.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040e6e0-945d-4779-bb60-bf2dfcb6c5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed504cf4-bd06-4fd6-9d41-d6ec756f7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e14c6-52f7-4ffd-a6f8-7f93133839e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0fc503-ee26-4230-a0ab-a11efcc01429",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab2457-bedb-4657-b1e7-99a3bd341f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.wv.expandos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd50cc-4906-4878-b238-e591c2949bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interested=ats[18:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a31a41-3757-40b7-b2c2-6b9881367825",
   "metadata": {},
   "outputs": [],
   "source": [
    "interested=interested[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651f867-8b75-4473-a57a-43bb5028e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "interested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d88eac7-70f5-4132-ae47-aa053d541a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9cf86-e621-4f5f-82dd-e0041420ea8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
